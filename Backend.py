# -*- coding: utf-8 -*-
"""Copy of *Project*

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j_UvDztRnfY5uGhxk_MyFxdZK8zqyyUa

# ***Firefly Optimization using 3 tier model***
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.metrics import accuracy_score

# Load the dataset
data = pd.read_csv('/content/resampled_data_x.csv')

# Separate features and target variable
X = data.drop('Class', axis=1)
y = data['Class']

# Encode categorical variables
label_encoder = LabelEncoder()
categorical_columns = X.select_dtypes(include=['object']).columns
X[categorical_columns] = X[categorical_columns].apply(label_encoder.fit_transform)

# Split the dataset into training and testing sets
X_train_base, X_test, y_train_base, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_base)
X_test_scaled = scaler.transform(X_test)

# Define Firefly Optimization for feature selection
def firefly_optimization(X_train, y_train, min_selected_features=10, max_iter=100, alpha=1.0, beta_min=0.2, gamma=1.0):
    num_features = X_train.shape[1]
    best_solution = None
    best_fitness = -np.inf

    while True:
        # Initialize fireflies
        fireflies = np.random.rand(num_features)

        # Evaluate fitness for each firefly
        selected_features_indices = np.where(fireflies > 0.5)[0]
        if len(selected_features_indices) < min_selected_features:
            continue
        clf = RandomForestClassifier(n_estimators=100, random_state=42)
        X_train_selected = X_train[:, selected_features_indices]
        clf.fit(X_train_selected, y_train)
        feature_importances = clf.feature_importances_

        # Select top features based on importance scores
        top_feature_indices = np.argsort(feature_importances)[::-1][:min_selected_features]
        selected_features_indices = selected_features_indices[top_feature_indices]

        # Update best solution
        fitness = accuracy_score(y_train, clf.predict(X_train_selected))
        if fitness > best_fitness:
            best_fitness = fitness
            best_solution = selected_features_indices

        # Check if at least 10 features are selected
        if len(selected_features_indices) >= min_selected_features:
            break

    return best_solution, best_fitness

# Perform Firefly Optimization for feature selection
selected_feature_indices, accuracy = firefly_optimization(X_train_scaled, y_train_base, min_selected_features=10)
selected_features = X.columns[selected_feature_indices].tolist()
print("Selected Features:", selected_features)
print("Accuracy after feature selection (Base Level):", accuracy)

 # Get feature importance scores for all features
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train_scaled, y_train_base)
feature_importances = clf.feature_importances_

# Output feature ranking
sorted_indices = np.argsort(feature_importances)
sorted_features = X.columns[sorted_indices]

print("\nFeature Ranking based on Importance Scores:")
for rank, feature in enumerate(sorted_features, start=1):
    print(f"{rank}. {feature}: {feature_importances[sorted_indices[rank - 1]]}")

# Intermediate Level Model
X_train_intermediate = X_train_base[selected_features]

# Top Level Model (Voting Classifier)
top_model = VotingClassifier(
    estimators=[
        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
        ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42))
    ],
    voting='soft'
)

# Train top model using intermediate-level predictions
top_model.fit(X_train_intermediate, y_train_base)

# Make predictions on the test set
X_test_intermediate = X_test[selected_features]
predictions_top_model = top_model.predict(X_test_intermediate)

# Evaluate the ensemble model
ensemble_accuracy = accuracy_score(y_test, predictions_top_model)
print(f'\nEnsemble Model Accuracy: {ensemble_accuracy}')

# Save selected features to CSV file
df_selected_features = data[selected_features + ['Class']]
df_selected_features.to_csv('selected_features_dataset(FF).csv', index=False)

k=pd.read_csv('selected_features_dataset(FF).csv')
print(k)

"""# ***R1 REGULARIZATION***"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LassoCV

# Read the dataset
df = pd.read_csv('selected_features_dataset(FF).csv')

# Separate numerical features
numerical_features = df.select_dtypes(include=['float64', 'int64'])

# Fill missing values if any
numerical_features.fillna(0, inplace=True)  # You can choose a different strategy for filling missing values

# Standardize numerical features
scaler = StandardScaler()
numerical_features_scaled = scaler.fit_transform(numerical_features)

# Instantiate LabelEncoder
label_encoder = LabelEncoder()

# Encode the target variable
df['target_encoded'] = label_encoder.fit_transform(df['Class'])

# Instantiate LassoCV model
lasso = LassoCV(cv=5)

# Fit the model
lasso.fit(numerical_features_scaled, df['target_encoded'])

# Print feature ranking scores
feature_ranking_scores = pd.Series(lasso.coef_, index=numerical_features.columns)
print("Feature Ranking Scores:")
print(feature_ranking_scores)

# Select top 4 features based on Lasso coefficients
feature_selector = SelectFromModel(lasso, max_features=4)

# Transform features
selected_features = feature_selector.fit_transform(numerical_features_scaled, df['target_encoded'])

# Get selected feature indices
selected_feature_indices = feature_selector.get_support(indices=True)

# Get selected feature names
selected_feature_names = numerical_features.columns[selected_feature_indices]

# Concatenate selected features with the target variable
selected_df = pd.concat([df[selected_feature_names], df['Class']], axis=1)

# Save new dataset to CSV
selected_df.to_csv('selected_features_dataset(R1).csv', index=False)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(selected_features, df['target_encoded'], test_size=0.2, random_state=42)

# Train base models (Random Forest Classifier) on different subsets of data
base_model1 = RandomForestClassifier()
base_model1.fit(X_train[:len(X_train)//2], y_train[:len(y_train)//2])

base_model2 = RandomForestClassifier()
base_model2.fit(X_train[len(X_train)//2:], y_train[len(y_train)//2:])

# Predictions from base models
predictions_base_model1 = base_model1.predict(X_test)
predictions_base_model2 = base_model2.predict(X_test)

# Stack the predictions
stacked_predictions = pd.DataFrame({
    'BaseModel1': predictions_base_model1,
    'BaseModel2': predictions_base_model2
})

# Train the meta-learner (Random Forest Classifier)
meta_learner = RandomForestClassifier()
meta_learner.fit(stacked_predictions, y_test)

# Combine predictions using meta-learner
meta_features = pd.DataFrame({
    'BaseModel1': base_model1.predict(X_test),
    'BaseModel2': base_model2.predict(X_test)
})
final_predictions = meta_learner.predict(meta_features)

# Calculate accuracy
accuracy = accuracy_score(y_test, final_predictions)

print("Selected Features using L1 Regularization (Lasso):")
print(selected_feature_names)
print("Accuracy:", accuracy)

k=pd.read_csv('/content/selected_features_dataset(R1).csv')
print(k)

"""# ***HYPERPARAMETER TUNNING***

# ***RANDOM SEARCH OPTIMIZATION USING GradientBoostingClassifier***
"""

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score
import pandas as pd

# Load the dataset from file
data = pd.read_csv('/content/selected_features_dataset(R1).csv')

# Separate features and target variable
X = data.drop(columns=['Class'])
y = data['Class']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the model
model = GradientBoostingClassifier()

# Define hyperparameter grid
param_dist = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Define RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_dist,
    n_iter=100,
    cv=5,
    scoring='accuracy',
    random_state=42,
    n_jobs=-1
)

# Fit the RandomizedSearchCV
random_search.fit(X_train, y_train)

# Get the results as a DataFrame
results_df = pd.DataFrame(random_search.cv_results_)

# Sort results by rank_test_score to have best results on top
results_df = results_df.sort_values(by='rank_test_score')

# Define the columns to display
columns_to_display = ['rank_test_score', 'mean_test_score', 'params']

# Display the top results
print("Top Results:")
print("+-------------+------------+----------------+-------------+---------------------+--------------------+")
print("| Trial No.   | Accuracy   | n_estimators  | max_depth   | min_samples_split  | min_samples_leaf   |")
print("+=============+============+================+=============+=====================+====================+")
for index, row in results_df[columns_to_display].head(10).iterrows():
    print("| {:<12}| {:<10.6f}| {:<15}| {:<12}| {:<20}| {:<19}|".format(
        index,
        row['mean_test_score'],
        row['params']['n_estimators'],
        row['params']['max_depth'],
        row['params']['min_samples_split'],
        row['params']['min_samples_leaf']
    ))
print("+-------------+------------+----------------+-------------+---------------------+--------------------+")

# Print the best hyperparameters found
best_hyperparams = random_search.best_params_
best_accuracy = accuracy_score(y_test, random_search.best_estimator_.predict(X_test))
print("| {:<12}| {:<10.6f}| {:<15}| {:<12}| {:<20}| {:<19}|".format(
    "Best",
    best_accuracy,
    best_hyperparams['n_estimators'],
    best_hyperparams['max_depth'],
    best_hyperparams['min_samples_split'],
    best_hyperparams['min_samples_leaf']
))
print("+-------------+------------+----------------+-------------+---------------------+--------------------+")
# Print the best hyperparameters found
print("\nBest Hyperparameters:")
print(random_search.best_params_)

# Predict on the test set with the best model
y_pred = random_search.best_estimator_.predict(X_test)

# Evaluate the model's accuracy
accuracy = accuracy_score(y_test, y_pred)
print("\nBest Test Accuracy:", accuracy)

"""# ***Population-Based Training using RandomForestClassifier***"""

!pip install optuna

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
import optuna
from tabulate import tabulate

# Load the dataset
df = pd.read_csv('/content/selected_features_dataset(R1).csv')

# Encode the target variable
df['Class'] = df['Class'].map({'H': 0, 'M': 1, 'L': 2})

# Split the data into features and target variable
X = df.drop('Class', axis=1)
y = df['Class']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the objective function
def objective(trial):
    # Define the search space
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 10, 200),
        'max_depth': trial.suggest_int('max_depth', 1, 20),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),
        'max_features': trial.suggest_int('max_features', 1, len(X.columns))
    }

    # Train the model
    clf = RandomForestClassifier(**params)
    clf.fit(X_train, y_train)

    # Evaluate the model
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    return accuracy

# Define the optimization study
study = optuna.create_study(direction='maximize')

# Start the optimization with Population-Based Training
study.optimize(objective, n_trials=100, timeout=None, n_jobs=-1)

# Store trial results in a list
results = []
for trial in study.trials:
    results.append([trial.number, trial.value] + [trial.params[param] for param in trial.params])

# Get best accuracy and corresponding best parameters
best_accuracy = study.best_value
best_params = study.best_trial.params

# Print results in tabular format
headers = ["Trial No.", "Accuracy"] + list(study.trials[0].params.keys())
table = tabulate(results, headers=headers, tablefmt="grid")

# Highlight the best parameters
highlighted_table = []
for row in table.split('\n'):
    if len(row.split()) > 1 and row.split()[1] == str(best_accuracy):
        row = '\033[1m' + row + '\033[0m'  # Highlight the row with best accuracy
    highlighted_table.append(row)

print('\n'.join(highlighted_table))

# Print best accuracy and corresponding best parameters
print("\nBest Accuracy:", best_accuracy)
print("Best Parameters:", best_params)

"""# ***Bayesian Optimization for Logistic Regression***"""

!pip install scikit-optimize

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve
from skopt import BayesSearchCV
from skopt.space import Real
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('/content/selected_features_dataset(R1).csv')

# Encode the target variable
df['Class'] = df['Class'].map({'H': 0, 'M': 1, 'L': 2})

# Split the data into features and target variable
X = df.drop('Class', axis=1)
y = df['Class']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the hyperparameter search space for Logistic Regression
lr_param_space = {
    'C': Real(0.1, 10)
}

# Perform Bayesian Optimization for Logistic Regression
lr_bo = BayesSearchCV(LogisticRegression(), lr_param_space, n_iter=50, cv=5, random_state=42, n_jobs=-1)
lr_bo.fit(X_train, y_train)

# Get the results as a DataFrame
results_df = pd.DataFrame(lr_bo.cv_results_)

# Print hyperparameters in every iteration
print("Hyperparameters in each iteration:")
print(results_df[['param_C', 'mean_test_score']])

# Get the best hyperparameters
best_lr_params = lr_bo.best_params_

# Evaluate the model with the best hyperparameters
best_lr_model = LogisticRegression(**best_lr_params)
best_lr_model.fit(X_train, y_train)
y_pred = best_lr_model.predict(X_test)

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred)

print(f'\nBest Accuracy: {accuracy}, Best Hyperparameters: {best_lr_params}')

"""# ***Three hyperparameter Tunnings combined one***"""

!pip install optuna

!pip install scikit-optimize

import pandas as pd
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve
import optuna
from tabulate import tabulate
from skopt import BayesSearchCV
from skopt.space import Real
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder

# Load the dataset
data = pd.read_csv('/content/selected_features_dataset(R1).csv')

# Separate features and target variable
X = data.drop(columns=['Class'])
y = data['Class']

# Encode the target variable if it contains string labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Randomized Search Optimization using GradientBoostingClassifier

# Define the model
model_gb = GradientBoostingClassifier()

# Define hyperparameter grid
param_dist_gb = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Define RandomizedSearchCV
random_search_gb = RandomizedSearchCV(
    estimator=model_gb,
    param_distributions=param_dist_gb,
    n_iter=100,
    cv=5,
    scoring='accuracy',
    random_state=42,
    n_jobs=-1
)

# Fit the RandomizedSearchCV
random_search_gb.fit(X_train, y_train)

# Get the results as a DataFrame
results_df_gb = pd.DataFrame(random_search_gb.cv_results_)

# Sort results by rank_test_score to have best results on top
results_df_gb = results_df_gb.sort_values(by='rank_test_score')

# Display the top results
print("Top Results for Gradient Boosting Classifier:")
print(tabulate(results_df_gb[['rank_test_score', 'mean_test_score', 'params']].head(10), headers='keys', tablefmt='psql'))

# Print the best hyperparameters found
best_hyperparams_gb = random_search_gb.best_params_
best_accuracy_gb = accuracy_score(y_test, random_search_gb.best_estimator_.predict(X_test))
print("\nBest Hyperparameters for Gradient Boosting Classifier:")
print(best_hyperparams_gb)
print("Best Test Accuracy for Gradient Boosting Classifier:", best_accuracy_gb)


# Population-Based Training using RandomForestClassifier

# Define the objective function
def objective_rf(trial):
    # Define the search space
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 10, 200),
        'max_depth': trial.suggest_int('max_depth', 1, 20),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),
        'max_features': trial.suggest_int('max_features', 1, len(X.columns))
    }

    # Train the model
    clf = RandomForestClassifier(**params)
    clf.fit(X_train, y_train)

    # Evaluate the model
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    return accuracy

# Define the optimization study
study_rf = optuna.create_study(direction='maximize')

# Start the optimization with Population-Based Training
study_rf.optimize(objective_rf, n_trials=100, timeout=None, n_jobs=-1)

# Store trial results in a list
results_rf = []
for trial in study_rf.trials:
    results_rf.append([trial.number, trial.value] + [trial.params[param] for param in trial.params])

# Get best accuracy and corresponding best parameters
best_accuracy_rf = study_rf.best_value
best_params_rf = study_rf.best_trial.params

# Print results in tabular format
headers_rf = ["Trial No.", "Accuracy"] + list(study_rf.trials[0].params.keys())
table_rf = tabulate(results_rf, headers=headers_rf, tablefmt="psql")

# Print best accuracy and corresponding best parameters
print("\nResults for Random Forest Classifier:")
print(table_rf)
print("\nBest Accuracy for Random Forest Classifier:", best_accuracy_rf)
print("Best Parameters for Random Forest Classifier:", best_params_rf)


# Bayesian Optimization for Logistic Regression

# Define the hyperparameter search space for Logistic Regression
lr_param_space = {
    'C': Real(0.1, 10)
}

# Perform Bayesian Optimization for Logistic Regression
lr_bo = BayesSearchCV(LogisticRegression(), lr_param_space, n_iter=50, cv=5, random_state=42, n_jobs=-1)
lr_bo.fit(X_train, y_train)

# Get the results as a DataFrame
results_df_lr = pd.DataFrame(lr_bo.cv_results_)

# Get the best hyperparameters
best_lr_params = lr_bo.best_params_

# Evaluate the model with the best hyperparameters
best_lr_model = LogisticRegression(**best_lr_params)
best_lr_model.fit(X_train, y_train)
y_pred_lr = best_lr_model.predict(X_test)

# Calculate evaluation metrics
accuracy_lr = accuracy_score(y_test, y_pred_lr)

# Print hyperparameters in every iteration
print("\nHyperparameters in each iteration for Logistic Regression:")
print(results_df_lr[['param_C', 'mean_test_score']])

# Print best accuracy and corresponding best parameters
print("\nBest Accuracy for Logistic Regression:", accuracy_lr)
print("Best Hyperparameters for Logistic Regression:", best_lr_params)

# Ensemble Model using Voting Classifier

# Initialize individual models with their best hyperparameters
best_gb_model = random_search_gb.best_estimator_
best_rf_model = RandomForestClassifier(**best_params_rf)
best_lr_model = LogisticRegression(**best_lr_params)

# Create a list of tuples containing model names and their corresponding models
models = [('GradientBoosting', best_gb_model), ('RandomForest', best_rf_model), ('LogisticRegression', best_lr_model)]

# Initialize a VotingClassifier with the individual models
voting_classifier = VotingClassifier(models, voting='soft')

# Fit the ensemble model
voting_classifier.fit(X_train, y_train)

# Predictions on the test set
y_pred_ensemble = voting_classifier.predict(X_test)

# Predicted probabilities for each class
y_pred_prob_ensemble = voting_classifier.predict_proba(X_test)

# Calculate evaluation metrics for the ensemble model
accuracy_ensemble = accuracy_score(y_test, y_pred_ensemble)
precision_ensemble = precision_score(y_test, y_pred_ensemble, average='weighted')
recall_ensemble = recall_score(y_test, y_pred_ensemble, average='weighted')
f1_ensemble = f1_score(y_test, y_pred_ensemble, average='weighted')
auc_ensemble = roc_auc_score(y_test, y_pred_prob_ensemble, multi_class='ovr')  # One-vs-Rest (OvR) strategy

# Print Evaluation Metrics for the Ensemble Model
print("\nEvaluation Metrics for the Ensemble Model:")
print("Accuracy:", accuracy_ensemble)
print("Precision:", precision_ensemble)
print("Recall:", recall_ensemble)
print("F1 Score:", f1_ensemble)
print("AUC Score:", auc_ensemble)

# Plot ROC Curve for the ensemble model (One-vs-Rest)
plt.figure(figsize=(8, 6))
for i in range(len(label_encoder.classes_)):
    fpr, tpr, _ = roc_curve(y_test, y_pred_prob_ensemble[:, i], pos_label=label_encoder.classes_[i])
    plt.plot(fpr, tpr, label=f'ROC Curve - Class {label_encoder.classes_[i]}')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Multiclass (One-vs-Rest)')
plt.legend(loc="lower right")
plt.show()